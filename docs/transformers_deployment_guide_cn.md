# ğŸš€ MiniMax æ¨¡å‹ Transformers éƒ¨ç½²æŒ‡å—

## ğŸ“– ç®€ä»‹

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨ä½¿ç”¨ [Transformers](https://huggingface.co/docs/transformers/index) åº“éƒ¨ç½² MiniMax-Text-01 æ¨¡å‹ã€‚Transformers æ˜¯ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ åº“ï¼Œæä¾›äº†ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹é›†åˆå’Œçµæ´»çš„æ¨¡å‹æ“ä½œæ¥å£ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

- ğŸ”¥ ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿï¼šé€šè¿‡ Hugging Face æ¨¡å‹ä¸­å¿ƒæä¾›æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒå„ç§ NLP ä»»åŠ¡ã€‚
- âš¡ ç»Ÿä¸€çš„ APIï¼šé€šè¿‡è·¨æ¶æ„çš„ä¸€è‡´æ¥å£ç®€åŒ–å¾®è°ƒå’Œæ¨ç†ï¼Œé™ä½å¼€å‘å¤æ‚æ€§ã€‚
- ğŸ“¦ å¤šè¯­è¨€å’Œè·¨æ¨¡æ€ï¼šæ”¯æŒå…¨çƒè¯­è¨€å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚å›¾åƒ-æ–‡æœ¬ï¼‰ï¼Œå®ç°å¤šæ ·åŒ–åº”ç”¨ã€‚
- âš™ï¸ ä¼˜åŒ–æ€§èƒ½ï¼šä¸ PyTorch/TensorFlow ç­‰æ¡†æ¶ä»¥åŠ vLLM ç­‰å·¥å…·é›†æˆï¼Œå®ç°é«˜æ•ˆéƒ¨ç½²å’Œæ‰©å±•ã€‚

## ç‰¹åˆ«æ„Ÿè°¢ 

ç‰¹åˆ«æ„Ÿè°¢ [Shakib](https://www.linkedin.com/in/shakibkhan66/) å’Œ [Armaghan](https://www.linkedin.com/in/armaghan-shakir/overlay/about-this-profile/) å¸®åŠ© MiniMax å¼€å‘äº† Transformers çš„ç®—å­æ”¯æŒã€‚

å…³äº Transformers çš„å¦‚ä½•ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥å‚è€ƒ Transformers çš„[æ¨¡å‹æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/model_doc/minimax)ã€‚

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£… Transformers

```bash
pip install transformers torch accelerate
```

## ğŸ“‹ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

å› ä¸ºç›®å‰æˆ‘ä»¬è¿˜æœªä¸Šä¼  MiniMaxAI/MiniMax-Text-01-hf çš„ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹configã€‚

é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

messages = [
 {"role": "user", "content": "What is your favourite condiment?"},
 {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
 {"role": "user", "content": "Do you have mayonnaise recipes?"}
]

text = tokenizer.apply_chat_template(
 messages,
 tokenize=False,
 add_generation_prompt=True
)

model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

generation_config = GenerationConfig(
 max_new_tokens=20,
 eos_token_id=tokenizer.eos_token_id,
 use_cache=True,
)

generated_ids = model.generate(**model_inputs, generation_config=generation_config)

generated_ids = [
 output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### ä½¿ç”¨ Flash Attention åŠ é€Ÿ

ä¸Šé¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†ä¸ä½¿ç”¨ä»»ä½•ä¼˜åŒ–æŠ€å·§çš„æ¨ç†è¿‡ç¨‹ã€‚ä½†é€šè¿‡åˆ©ç”¨ [Flash Attention](../perf_train_gpu_one#flash-attention-2)ï¼Œå¯ä»¥å¤§å¹…åŠ é€Ÿæ¨¡å‹ï¼Œå› ä¸ºå®ƒæä¾›äº†æ¨¡å‹å†…éƒ¨ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«å®ç°ã€‚

é¦–å…ˆï¼Œç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Flash Attention 2 ä»¥åŒ…å«æ»‘åŠ¨çª—å£æ³¨æ„åŠ›åŠŸèƒ½ï¼š

```bash
pip install -U flash-attn --no-build-isolation
```

è¿˜è¦ç¡®ä¿æ‚¨æ‹¥æœ‰ä¸ Flash-Attention 2 å…¼å®¹çš„ç¡¬ä»¶ã€‚åœ¨[Flash Attention å®˜æ–¹ä»“åº“](https://github.com/Dao-AILab/flash-attention)çš„å®˜æ–¹æ–‡æ¡£ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯·ç¡®ä¿ä»¥åŠç²¾åº¦ï¼ˆä¾‹å¦‚ `torch.float16`ï¼‰åŠ è½½æ¨¡å‹ã€‚

è¦ä½¿ç”¨ Flash Attention-2 åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

prompt = "My favourite condiment is"

model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)
```

## ğŸ“® è·å–æ”¯æŒ

å¦‚æœæ‚¨åœ¨éƒ¨ç½² MiniMax-Text-01 æ¨¡å‹è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼š
- è¯·æŸ¥çœ‹æˆ‘ä»¬çš„å®˜æ–¹æ–‡æ¡£
- é€šè¿‡å®˜æ–¹æ¸ é“è”ç³»æˆ‘ä»¬çš„æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ
- åœ¨æˆ‘ä»¬çš„ GitHub ä»“åº“æäº¤ Issue

æˆ‘ä»¬ä¼šæŒç»­ä¼˜åŒ– Transformers ä¸Šçš„éƒ¨ç½²ä½“éªŒï¼Œæ¬¢è¿æ‚¨çš„åé¦ˆï¼